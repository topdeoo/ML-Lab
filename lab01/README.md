# Lab 01 Linear Regression

## 实验环境

使用 `jupyter` 完成实验，具体而言，你只需要补全 `lab.ipynb` 中的代码即可

## 实验数据

使用了 `kaggle` 上的数据：[数据集](https://www.kaggle.com/datasets/hellbuoy/car-price-prediction)

是一个关于汽车价格预测的任务，数据的预处理已在 `lab.ipynb` 中完成，不需要担心这个的问题

数据集含义的解释可以在 `kaggle` 上查看

## 实验任务

编写一个名为 `LinearRegression` 的类。这个类能够通过 `fit` 方法来训练线性回归模型，通过 `predict` 方法预测给定数据集的结果。

在这里我们的 `Loss` 函数取 `MSE`，由于 `MSE` 的计算量会很大，我对一些数据已经进行了一些缩小处理，请不需要担心这一点。

这个类还需要记录每次迭代时 `Loss` 的值与每次迭代时函数梯度的二范数（这在判断模型是否成功有着关键的作用）

最后，你需要测试这个类，具体而言你将：
1. 观察 `Loss` 的图像是否在降低并且逐渐趋向于 $0$
2. 观察梯度的二范数的图像，判断当 `Loss` 的值趋于稳定时，梯度是否趋向于 $0$
3. 调用 `predict` 方法对测试集进行测试，计算预测值与真实值之间的 `MSE` 并输出，并且你还需要计算模型的 $R^2$，这里给出计算公式：
   $$
   R^2 = \frac{\sum_{i=1}^N(\hat{y}_i - \bar{y})^2}{\sum^N_{i=1}(y_i - \bar{y})^2}
   $$
   其中，$\hat{y}_i$ 为估计值，$\bar{y}$ 为真实值的均值

> tips: 你可以使用 `sklearn` 中的线性回归与你自己实现的对比

## 进阶内容

1. 如果你发现你的模型存在过拟合迹象，请尝试使用结构风险最小化函数来解决，具体而言你将加入一个正则化项：
   $\lambda \frac{1}{2}(||w||^2_2 + b)
   其中 $\lambda$ 是一个超参数，代表着惩罚力度
2. 如果你认为你的模型卡在了驻点而不是真的到达了全局最小，请尝试优化你的迭代方法，可以实现一个简单的 `RMSProp` 算法
